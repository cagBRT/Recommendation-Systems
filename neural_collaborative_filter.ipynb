{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Recommendation-Systems/blob/main/neural_collaborative_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAMAmBkKkUgr"
      },
      "source": [
        "# Recommender systems - Neural Collaborative Filtering\n",
        "> Demo\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- hide: true\n",
        "- categories: [demo, neural networks, deep learning, recommender systems, paper]\n",
        "- image: https://raw.githubusercontent.com/murilo-cunha/inteligencia-superficial/master/images/2020-09-11-neural_collaborative_filter/cover.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSgfymkRkUgt"
      },
      "source": [
        "Download dependencies and run `tensorboard` in the background:\n",
        "\n",
        "```python\n",
        "!pip install tensorflow lightfm pandas\n",
        "```\n",
        "\n",
        "```python\n",
        "%load_ext tensorboard\n",
        "!tensorboard --logdir 2020-09-11-neural_collaborative_filter/logs &\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightfm"
      ],
      "metadata": {
        "id": "kngSf7H3kfUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnxERls9kUgt"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import lightfm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from lightfm import LightFM\n",
        "from lightfm.datasets import fetch_movielens\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y1E6a94kUgu"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "print(f\"LightFM version: {lightfm.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkcCla1JkUgu"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "TOP_K = 5\n",
        "N_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "bSw2k7FgkUgu"
      },
      "source": [
        "# Data\n",
        "\n",
        "![](https://raw.githubusercontent.com/murilo-cunha/inteligencia-superficial/master/images/2020-09-11-neural_collaborative_filter/matrix_factorization_with_alpha.png \"Credit: https://developers.google.com/machine-learning/recommendation/collaborative/basics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0LTG9kzkUgv"
      },
      "outputs": [],
      "source": [
        "# hide_input\n",
        "data = fetch_movielens(min_rating=3.0)\n",
        "\n",
        "\n",
        "print(\"Interaction matrix:\")\n",
        "print(data[\"train\"].toarray()[:20, :20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIXT9FikkUgv"
      },
      "outputs": [],
      "source": [
        "# collapse\n",
        "for dataset in [\"test\", \"train\"]:\n",
        "    data[dataset] = (data[dataset].toarray() > 0).astype(\"int8\")\n",
        "\n",
        "# Make the ratings binary\n",
        "print(\"Interaction matrix:\")\n",
        "print(data[\"train\"][:10, :10])\n",
        "\n",
        "print(\"\\nRatings:\")\n",
        "unique_ratings = np.unique(data[\"train\"])\n",
        "print(unique_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kla0-jvkUgv"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def wide_to_long(wide: np.array, possible_ratings: List[int]) -> np.array:\n",
        "    \"\"\"Go from wide table to long.\n",
        "    :param wide: wide array with user-item interactions\n",
        "    :param possible_ratings: list of possible ratings that we may have.\"\"\"\n",
        "\n",
        "    def _get_ratings(arr: np.array, rating: int) -> np.array:\n",
        "        \"\"\"Generate long array for the rating provided\n",
        "        :param arr: wide array with user-item interactions\n",
        "        :param rating: the rating that we are interested\"\"\"\n",
        "        idx = np.where(arr == rating)\n",
        "        return np.vstack(\n",
        "            (idx[0], idx[1], np.ones(idx[0].size, dtype=\"int8\") * rating)\n",
        "        ).T\n",
        "\n",
        "    long_arrays = []\n",
        "    for r in possible_ratings:\n",
        "        long_arrays.append(_get_ratings(wide, r))\n",
        "\n",
        "    return np.vstack(long_arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcQQ96_ikUgv"
      },
      "outputs": [],
      "source": [
        "long_train = wide_to_long(data[\"train\"], unique_ratings)\n",
        "df_train = pd.DataFrame(long_train, columns=[\"user_id\", \"item_id\", \"interaction\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxjsqVGNkUgw"
      },
      "outputs": [],
      "source": [
        "# hide_input\n",
        "print(\"All interactions:\")\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1RgqAGDkUgw"
      },
      "outputs": [],
      "source": [
        "# hide_input\n",
        "print(\"Only positive interactions:\")\n",
        "df_train[df_train[\"interaction\"] > 0].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qtYT61lkUgw"
      },
      "source": [
        "# The model (Neural Collaborative Filtering)\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/murilo-cunha/inteligencia-superficial/master/images/2020-09-11-neural_collaborative_filter/ncf_all_with_alpha.png\" width=\"70%\" url=\"https://developers.google.com/machine-learning/recommendation/collaborative/basics\" description=\"Fonte: https://developers.google.com/machine-learning/recommendation/collaborative/basics\" /> </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShoowlvZkUgw"
      },
      "outputs": [],
      "source": [
        "import keras as keras\n",
        "from keras.layers import (\n",
        "    Concatenate,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    Input,\n",
        "    Multiply,\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "def create_ncf(\n",
        "    number_of_users: int,\n",
        "    number_of_items: int,\n",
        "    latent_dim_mf: int = 4,\n",
        "    latent_dim_mlp: int = 32,\n",
        "    reg_mf: int = 0,\n",
        "    reg_mlp: int = 0.01,\n",
        "    dense_layers: List[int] = [8, 4],\n",
        "    reg_layers: List[int] = [0.01, 0.01],\n",
        "    activation_dense: str = \"relu\",\n",
        ") -> keras.Model:\n",
        "\n",
        "    # input layer\n",
        "    user = Input(shape=(), dtype=\"int32\", name=\"user_id\")\n",
        "    item = Input(shape=(), dtype=\"int32\", name=\"item_id\")\n",
        "\n",
        "    # embedding layers\n",
        "    mf_user_embedding = Embedding(\n",
        "        input_dim=number_of_users,\n",
        "        output_dim=latent_dim_mf,\n",
        "        name=\"mf_user_embedding\",\n",
        "        embeddings_initializer=\"RandomNormal\",\n",
        "        embeddings_regularizer=l2(reg_mf),\n",
        "        input_length=1,\n",
        "    )\n",
        "    mf_item_embedding = Embedding(\n",
        "        input_dim=number_of_items,\n",
        "        output_dim=latent_dim_mf,\n",
        "        name=\"mf_item_embedding\",\n",
        "        embeddings_initializer=\"RandomNormal\",\n",
        "        embeddings_regularizer=l2(reg_mf),\n",
        "        input_length=1,\n",
        "    )\n",
        "\n",
        "    mlp_user_embedding = Embedding(\n",
        "        input_dim=number_of_users,\n",
        "        output_dim=latent_dim_mlp,\n",
        "        name=\"mlp_user_embedding\",\n",
        "        embeddings_initializer=\"RandomNormal\",\n",
        "        embeddings_regularizer=l2(reg_mlp),\n",
        "        input_length=1,\n",
        "    )\n",
        "    mlp_item_embedding = Embedding(\n",
        "        input_dim=number_of_items,\n",
        "        output_dim=latent_dim_mlp,\n",
        "        name=\"mlp_item_embedding\",\n",
        "        embeddings_initializer=\"RandomNormal\",\n",
        "        embeddings_regularizer=l2(reg_mlp),\n",
        "        input_length=1,\n",
        "    )\n",
        "\n",
        "    # MF vector\n",
        "    mf_user_latent = Flatten()(mf_user_embedding(user))\n",
        "    mf_item_latent = Flatten()(mf_item_embedding(item))\n",
        "    mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
        "\n",
        "    # MLP vector\n",
        "    mlp_user_latent = Flatten()(mlp_user_embedding(user))\n",
        "    mlp_item_latent = Flatten()(mlp_item_embedding(item))\n",
        "    mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "\n",
        "    mlp_vector = mlp_cat_latent\n",
        "\n",
        "    # build dense layers for model\n",
        "    for i in range(len(dense_layers)):\n",
        "        layer = Dense(\n",
        "            dense_layers[i],\n",
        "            activity_regularizer=l2(reg_layers[i]),\n",
        "            activation=activation_dense,\n",
        "            name=\"layer%d\" % i,\n",
        "        )\n",
        "        mlp_vector = layer(mlp_vector)\n",
        "\n",
        "    predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
        "\n",
        "    result = Dense(\n",
        "        1, activation=\"sigmoid\", kernel_initializer=\"lecun_uniform\", name=\"interaction\"\n",
        "    )\n",
        "\n",
        "    output = result(predict_layer)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[user, item],\n",
        "        outputs=[output],\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuOObIVOkUgw"
      },
      "outputs": [],
      "source": [
        "# collapse\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "n_users, n_items = data[\"train\"].shape\n",
        "ncf_model = create_ncf(n_users, n_items)\n",
        "\n",
        "ncf_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        tf.keras.metrics.TruePositives(name=\"tp\"),\n",
        "        tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
        "        tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "        tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "        tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\"),\n",
        "        tf.keras.metrics.AUC(name=\"auc\"),\n",
        "    ],\n",
        ")\n",
        "ncf_model._name = \"neural_collaborative_filtering\"\n",
        "ncf_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "278penflkUgw"
      },
      "outputs": [],
      "source": [
        "def make_tf_dataset(\n",
        "    df: pd.DataFrame,\n",
        "    targets: List[str],\n",
        "    val_split: float = 0.1,\n",
        "    batch_size: int = 512,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Make TensorFlow dataset from Pandas DataFrame.\n",
        "    :param df: input DataFrame - only contains features and target(s)\n",
        "    :param targets: list of columns names corresponding to targets\n",
        "    :param val_split: fraction of the data that should be used for validation\n",
        "    :param batch_size: batch size for training\n",
        "    :param seed: random seed for shuffling data - `None` won't shuffle the data\"\"\"\n",
        "\n",
        "    n_val = round(df.shape[0] * val_split)\n",
        "    if seed:\n",
        "        # shuffle all the rows\n",
        "        x = df.sample(frac=1, random_state=seed).to_dict(\"series\")\n",
        "    else:\n",
        "        x = df.to_dict(\"series\")\n",
        "    y = dict()\n",
        "    for t in targets:\n",
        "        y[t] = x.pop(t)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "\n",
        "    ds_val = ds.take(n_val).batch(batch_size)\n",
        "    ds_train = ds.skip(n_val).batch(batch_size)\n",
        "    return ds_train, ds_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4LUpcqJkUgw"
      },
      "outputs": [],
      "source": [
        "# create train and validation datasets\n",
        "ds_train, ds_val = make_tf_dataset(df_train, [\"interaction\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP-9LqlLkUgw",
        "outputId": "5fcdbf5a-b188-4b36-defb-ab70c4345da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2789/2789 [==============================] - 25s 9ms/step - loss: 0.1108 - tp: 16185.0000 - fp: 8140.0000 - tn: 1352274.0000 - fn: 50914.0000 - accuracy: 0.9586 - precision: 0.6654 - recall: 0.2412 - auc: 0.9344 - val_loss: 0.1131 - val_tp: 1868.0000 - val_fp: 922.0000 - val_tn: 150163.0000 - val_fn: 5660.0000 - val_accuracy: 0.9585 - val_precision: 0.6695 - val_recall: 0.2481 - val_auc: 0.9303\n",
            "Epoch 6/10\n",
            " 654/2789 [======>.......................] - ETA: 11s - loss: 0.1101 - tp: 4000.0000 - fp: 2047.0000 - tn: 316930.0000 - fn: 11871.0000 - accuracy: 0.9584 - precision: 0.6615 - recall: 0.2520 - auc: 0.9368"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# define logs and callbacks\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=0\n",
        ")\n",
        "\n",
        "train_hist = ncf_model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_val,\n",
        "    epochs=N_EPOCHS,\n",
        "    callbacks=[tensorboard_callback, early_stopping_callback],\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4INNDBwGkUgw"
      },
      "outputs": [],
      "source": [
        "long_test = wide_to_long(data[\"train\"], unique_ratings)\n",
        "df_test = pd.DataFrame(long_test, columns=[\"user_id\", \"item_id\", \"interaction\"])\n",
        "ds_test, _ = make_tf_dataset(df_test, [\"interaction\"], val_split=0, seed=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98gFTl_PkUgw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "ncf_predictions = ncf_model.predict(ds_test)\n",
        "df_test[\"ncf_predictions\"] = ncf_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPUrbkejkUgx"
      },
      "outputs": [],
      "source": [
        "# hide_input\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_y2WWOSkUgx"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "# sanity checks\n",
        "# stop execution if low standard deviation (all recommendations are the same)\n",
        "std = df_test.describe().loc[\"std\", \"ncf_predictions\"]\n",
        "if std < 0.01:\n",
        "    raise ValueError(\"Model predictions have standard deviation of less than 1e-2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp3RGH2SkUgx"
      },
      "outputs": [],
      "source": [
        "# collapse\n",
        "data[\"ncf_predictions\"] = df_test.pivot(\n",
        "    index=\"user_id\", columns=\"item_id\", values=\"ncf_predictions\"\n",
        ").values\n",
        "print(\"Neural collaborative filtering predictions\")\n",
        "print(data[\"ncf_predictions\"][:10, :4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4YVVZ2ckUgx"
      },
      "outputs": [],
      "source": [
        "precision_ncf = tf.keras.metrics.Precision(top_k=TOP_K)\n",
        "recall_ncf = tf.keras.metrics.Recall(top_k=TOP_K)\n",
        "\n",
        "precision_ncf.update_state(data[\"test\"], data[\"ncf_predictions\"])\n",
        "recall_ncf.update_state(data[\"test\"], data[\"ncf_predictions\"])\n",
        "print(\n",
        "    f\"At K = {TOP_K}, we have a precision of {precision_ncf.result().numpy():.5f}\",\n",
        "    \"and a recall of {recall_ncf.result().numpy():.5f}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocEIMgIxkUgx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# LightFM model\n",
        "def norm(x: float) -> float:\n",
        "    \"\"\"Normalize vector\"\"\"\n",
        "    return (x - np.min(x)) / np.ptp(x)\n",
        "\n",
        "\n",
        "lightfm_model = LightFM(loss=\"warp\")\n",
        "lightfm_model.fit(sparse.coo_matrix(data[\"train\"]), epochs=N_EPOCHS)\n",
        "\n",
        "lightfm_predictions = lightfm_model.predict(\n",
        "    df_test[\"user_id\"].values, df_test[\"item_id\"].values\n",
        ")\n",
        "df_test[\"lightfm_predictions\"] = lightfm_predictions\n",
        "wide_predictions = df_test.pivot(\n",
        "    index=\"user_id\", columns=\"item_id\", values=\"lightfm_predictions\"\n",
        ").values\n",
        "data[\"lightfm_predictions\"] = norm(wide_predictions)\n",
        "\n",
        "# compute the metrics\n",
        "precision_lightfm = tf.keras.metrics.Precision(top_k=TOP_K)\n",
        "recall_lightfm = tf.keras.metrics.Recall(top_k=TOP_K)\n",
        "precision_lightfm.update_state(data[\"test\"], data[\"lightfm_predictions\"])\n",
        "recall_lightfm.update_state(data[\"test\"], data[\"lightfm_predictions\"])\n",
        "print(\n",
        "    f\"At K = {TOP_K}, we have a precision of {precision_lightfm.result().numpy():.5f}\",\n",
        "    \"and a recall of {recall_lightfm.result().numpy():.5f}\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}